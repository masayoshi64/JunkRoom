# 2章

## エントロピー

$H(X) = \sum p(x)\log p(x )$

- $H\geq0$
- $H(X|Y)\leq H(X)$ 条件付けによるエントロピーの減少
- $H(X_1, \cdots, X_n)\leq \sum H(X_i)$
- $H\leq \log|\mathcal{X}|$
- $H$は上に凸





## 双対エントロピー = KLダイバージェンス

## 相互情報量

$$
I(X;Y) = \sum_{(x, y)}p(x, y)\log\frac{p(x, y)}{p(x)p(y)}
$$

- $I(X;Y) = H(X)-H(X|Y) = H(X)+H(Y)-H(X, Y)$

## チェイン則

- エントロピー

  $H(X_1, \cdots, X_n)=\sum H(X_i|X_{i-1},\cdots, X_1)$

- 相互情報量

  $H(X_1, \cdots, X_n;Y)=\sum H(X_i;Y|X_{i-1},\cdots, X_1)$

- 双対エントロピいー

  $D(p(x, y)||q(x, y))=D(p(x)||q(x))+D(p(y|x), q(y|x))$

## データ処理不等式

$X\to Y\to Z$がマルコフ連鎖をなすとき、$I(X;Y)\geq I(X;Z)$

## 十分統計量

$T(X)$が$\{f_\theta(x)\}$に対する十分統計量とは、任意の$\theta$について$I(\theta;X) = I(\theta;T(X))$となること。

## ファノの不等式

確率変数$Y$を知っている時、関連する確率変数$X$を推定したいとする。$\hat X(Y)$をその推定量とすると、これらは
$$
X\to Y \to \hat X(Y)
$$


なるマルコフ連鎖をなし、その誤り確率を$P_e = \mathrm{Pr}\{\hat X\neq X\}$とすると
$$
H(P_e)+P_e\log|\mathcal{X}|\geq H(X|Y)
$$
つまり、条件付きエントロピーに関連する量によって$P_e$は下から抑えられる

# 3章

## AEP.

「ほとんどすべての事象はほとんど等しい驚きを持つ」

具体的には、i.i.d.の確率変数列に対して
$$
\frac{1}{n} \log p(X_1, \cdots, X_n)\to H(X) （確率収束）
$$

## 典型集合

系列$x_1, \cdots, x_n$のうち、
$$
2^{-n(H(X)+\epsilon)}\leq p(x_1, \cdots, x_n)\leq 2^{-n(H(X)-\epsilon)}
$$
となる集合$A^{(n)}_\epsilon$。つまり、経験エントロピーが本当のエントロピーとほとんど等しいような系列の集合。これがすべての系列の（確率的な）多数派となる。

ただし要素数としては少数派である。

- 十分大きい$n$に対して$\mathrm{Pr}\{A^{(n)}_\epsilon\}>1-\epsilon$。つまり、大体典型列が出てくる。
- $|A^{(n)}_\epsilon|\leq 2^{n(H(X)+\epsilon)}$.　つまり、要素数は全体のうち$2^{nH}$ほど。

# 4章

## エントロピーレート

$$
H(\mathcal X) = \lim \frac{1}{n}H(X_1, \cdots, X_n),\\
H'(\mathcal X) = \lim H(X_n|X_{n-1}, \cdots, X_1).
$$

定常過程（時間シフトで同時確率分布が不変な過程）では両者は等しい。（不変性から条件付きエントロピーは単調減少かつ有界なので極限値が存在し、チェザロ平均の性質から成り立つ）

## 定常マルコフ連鎖に対するエントロピーレート

$$
H(\mathcal X) = -\sum_{i, j} \mu_i P_{ij} \log P_{ij}
$$

## 熱力学の第二法則

1. 任意の二つの分布について相対エントロピー$D(\mu_n||\mu_n')$は時間と共に減少する。つまり分布の区別がつきづらくなる。
2. 特に定常分布との相対エントロピーも減少する
3. 定常分布が一様分布なら相対エントロピーは$定数-H(X_n)$なのでエントロピーは増大する
4. 条件付きエントロピー$H(X_n|X_1)$は増大する
5. 逆に$H(X_0|X_n)$も増大する

## マルコフ連鎖の関数

# 第5章 データ圧縮

## クラフトの不等式

$$
瞬時符号\Leftrightarrow \sum D^{-l_i}\leq 1
$$

$\sum D^{l_{max}-l_i}\leq D^{l_{max}}$と考えた方がわかりやすい。treeを考えればわかる。

## マクミランの不等式

$$
一意復号可能\Leftrightarrow \sum D^{-l_i}\leq 1
$$

## データ圧縮におけるエントロピー限界

$$
L \equiv \sum p_il_i\geq H_D(X)
$$

マクミランの不等式の制約のもとで平均符号長を最大化する（符合長の整数条件を無視）ことでエントロピーが得られる（ラグランジュの未定乗数法による）。

## シャノン符号

$$
l_i = \lceil-\log_D p_i\rceil
$$

最適ではない

## ハフマン符号

確率の低い値から順にD個を一纏めにしていくことで得られる。

- 20の質問ゲームと同値である
- 最適である

## 不適切な符号

$X\sim p(x), l(x) = \lceil-\log_D q_i\rceil, L = \sum p(x)l(x)$の時
$$
H(p)+D(p||q)\leq L < H(p)+D(p||q)+1
$$


## 確率過程

$$
\frac{H(X_1, X_2, \dots, X_n)}{n}\leq L_n < \frac{H(X_1, X_2, \dots, X_n)+1}{n}
$$



## 定常過程

$$
\lim L_n = H(\mathcal X)
$$



## 競合最適性

シャノン符号$l_i = \lceil-\log_D p_i\rceil$と任意の符号$l'$の間に
$$
{\rm Pr}(l(X)\geq  l'(X)+c)\leq 2^{-c+1}
$$


# ギャンブルとデータ圧縮

## 倍増レート

$S(X) = b(X)o(X)$は馬$X$が買った時の資金の増加倍率。$b(X)$は掛け金で、$o(X)$はオッズ。 
$$
W(b, p) = E[\log S(X)] = \sum p_k \log b_k o_k
$$
を倍増レートという

## 最適な倍増レート

$$
W^*(p) = {\rm max}_b W(b, p)
$$



 ## 比例ギャンブリングは対数最適

$$
W^*(p) = {\rm max}_b W(b, p) = \sum p_i \log o_i - H(p)
$$

が$b^* = p$によって達成される。



## 成長率

富$S_n$は
$$
S_n = 2^{nW(p)}
$$
でふえる

## 保存則

公平なオッズ（$\sum \frac{1}{o_i} = 1$で胴元の取り分がない）場合、
$$
W(b, p) = D(p||r)-D(p||b)
$$
ただし$r_i = 1/o_i$（つまり、胴元の勝率予想）である。これはプレイヤーの予想$b$が胴元の予想$r_i$より良い時、利益が出ることを示している。

特にオッズが対等（$o_i =1/m$）の時、
$$
W^*(p)+H(p) = \log m
$$
と倍増レートとエントロピーレートの和が保存する。

## 補助情報

競馬Xで、補助情報$Y$が存在することによる倍増レートの増加は以下の通り。
$$
\Delta W = I(X;Y)
$$




